{% extends 'base.html' %}
{% block content %}

<div class="row">
  <div class="col s12 m12 l6">
    <div id="player">Loading...</div>
    <p class="center-align">Click video to start record!</p>
  </div>
  <div class="col s12 m12 l6">
    <form id="form" action="/yt/{{ video_id }}/create" method="post">
      <h6>Notes</h6>
      <canvas id="mainCanvas" width="480" height="80" style="margin: 0; padding: 0; border: 1px solid #9e9e9e"></canvas>
      <input type="hidden" id="notes" name="notes" value="">

      <div class="input-field">
        <input id="title" name="title" type="text" class="validate">
        <label for="title">Title</label>
      </div>

      <div class="input-field">
        <input id="nickname" name="nickname" type="text" class="validate">
        <label for="nickname">Nickame</label>
      </div>

      <input type="hidden" name="video_title" id="video_title" value="">

      <!-- <a href="#" class="waves-effect waves-light btn"><i class="material-icons left">cloud</i>Re</a> -->
      <a href="#" id="btn-submit" class="waves-effect waves-light btn"><i class="material-icons left">cloud</i>Submit</a>
    </form>
  </div>
</div>

<!-- <a class="btn-floating btn-large waves-effect waves-light red"><i class="material-icons">add</i></a> -->


{% endblock %}
{% block lazy_script %}
<script type="text/javascript" src="{{ url_for('static', filename='js/chroma.js') }}"></script>
<script type="text/javascript" src="{{ url_for('static', filename='js/recorder.js') }}"></script>
<script>
  var mainCanvas;
  var mainCtx;

  var record = false;
  var rec_item = {
    'notes': []
  }

  window.requestAnimFrame = (function(){
    return  window.requestAnimationFrame       ||
            window.webkitRequestAnimationFrame ||
            window.mozRequestAnimationFrame    ||
            window.oRequestAnimationFrame      ||
            window.msRequestAnimationFrame     ||
            function(callback, element){
              window.setTimeout(callback, 1000 / 60);
            };
  })();


  // Global Variables for Audio
  var audioContext;

  var sourceNode;
  var analyserNode;
  var javascriptNode;
  var playbackSourceNode;
  var audioStream;
  var array = [];

  var recording = null;  // this is the cumulative buffer for your recording

  var audioBufferNode = null;
  var audioBuffer = null;

  // Global Variables for Drawing
  var x = 0;
  var canvasWidth  = 800;
  var canvasHeight = 256;

  window.craicAudioContext = (function(){
    return  window.AudioContext || window.AudioContext ;
  })();

  navigator.getMedia = ( navigator.mozGetUserMedia ||
    navigator.getUserMedia ||
    navigator.webkitGetUserMedia ||
    navigator.msGetUserMedia);

  var start_time, last_time = 0;
  var timerId = 0;
  var playing = false;

  function startGame() {
    console.log('startGame()');
    start_time = Date.now()
    // document.getElementById('audioPlayer').play()

    // execute every time a new sample has been acquired
    javascriptNode.onaudioprocess = function (e) {
      addSampleToRecording(e.inputBuffer);

      // Analyze the frequencies in this sample and add to the spectorgram
      analyserNode.getByteFrequencyData(array);
      requestAnimFrame(draw);
    }

    playing = true;
    timerId = setInterval('process()', 50);
  }

  function stopGame() {
    playing = false;
    if (timerId != 0) {
      clearInterval(timerId);
      javascriptNode.onaudioprocess = null;
    }
  }

  var lastPauseTime = 0;
  var pausedTimes = 0;
  function pauseGame() {
    playing = false;
    if (timerId != 0) {
      clearInterval(timerId);
      lastPauseTime = Date.now();
    }
  }

  function resumeGame() {
    playing = true;
    pausedTimes += Date.now() - lastPauseTime;
    timerId = setInterval('process()', 50);
  }

  $(document).ready(function() {
    resizeCanvas();

    mainCanvas = document.getElementById('mainCanvas');
    mainCtx = $("#mainCanvas").get()[0].getContext("2d");
    clearCanvas();

    // Check that the browser can handle web audio
    try {
      // audioContext = new webkitAudioContext();
      audioContext = new craicAudioContext();
    }
    catch(e) {
      alert('Web Audio API is not supported in this browser');
    }

    // get the input audio stream and set up the nodes
    try {
      // calls the function setupAudioNodes
      //            navigator.webkitGetUserMedia({audio:true}, setupAudioNodes, onError);
      navigator.getMedia({audio:true}, setupAudioNodes, onError);
    } catch (e) {
      alert('webkitGetUserMedia threw exception :' + e);
    }

    $('#btn-submit').click(function() {
      $('#notes').val("[" + rec_item.notes.toString() + "]");
      $('#form').submit();
    });

/*
    // Play the recording
    $("body").on('click', "#playback_button",function(e) {
      e.preventDefault();
      playRecording();
    });

    // Reset the recording buffer and the graphics, but keep the nodes connected
    $("body").on('click', "#reset_button",function(e) {
      e.preventDefault();
      recording = null;
      clearCanvas();
    });

    // Disable audio completely
    $("body").on('click', "#disable_audio",function(e) {
      e.preventDefault();
      javascriptNode.onaudioprocess = null;
      if(audioStream)  audioStream.stop();
      if(sourceNode)  sourceNode.disconnect();
    });
*/
  });

  function onError(e) {
    console.log(e);
  }

  // Add this buffer to the recording
  // recording is a global
  function addSampleToRecording(inputBuffer) {
    var currentBuffer = inputBuffer.getChannelData(0);

    if (recording ==  null) {
      // handle the first buffer
      recording = currentBuffer;
    } else {
      // allocate a new Float32Array with the updated length
      newlen = recording.length + currentBuffer.length;
      var newBuffer = new Float32Array(newlen);
      newBuffer.set(recording, 0);
      newBuffer.set(currentBuffer, recording.length);
      recording = newBuffer;
    }
  }

  function setupAudioNodes(stream) {
  var sampleSize = 1024;  // number of samples to collect before analyzing FFT
      // decreasing this gives a faster sonogram, increasing it slows it down
      audioStream = stream;

    // The nodes are:  sourceNode -> analyserNode -> javascriptNode -> destination

    // create an audio buffer source node
    sourceNode = audioContext.createMediaStreamSource(audioStream);

    // Set up the javascript node - this uses only one channel - i.e. a mono microphone
    //javascriptNode = audioContext.createJavaScriptNode(sampleSize, 1, 1);
    javascriptNode = audioContext.createScriptProcessor(sampleSize, 1, 1);

    // setup the analyser node
    analyserNode = audioContext.createAnalyser();
    analyserNode.smoothingTimeConstant = 0.0;
    analyserNode.fftSize = 1024; // must be power of two

    // connect the nodes together
    sourceNode.connect(analyserNode);
    analyserNode.connect(javascriptNode);
    javascriptNode.connect(audioContext.destination);

    // optional - connect input to audio output (speaker)
    // This will echo your input back to your speakers - Beware of Feedback !!
    // sourceNode.connect(audioContext.destination);

    // allocate the array for Frequency Data
    array = new Uint8Array(analyserNode.frequencyBinCount);
  }

  // Draw the Spectrogram from the frequency array
  // adapted from http://www.smartjava.org/content/exploring-html5-web-audio-visualizing-sound
  function draw() {
    if (playing) {
      width = mainCanvas.width;
      height = mainCanvas.height;

      mainCtx.clearRect(0, 0, width, height);
      tm = Date.now() - start_time - pausedTimes;

      // console.log(rec_item.notes)
      for (i in rec_item.notes) {
        // if (rec_item.notes[i] > tm + 2000) break;
        x = (tm - rec_item.notes[i]) / 10;

        mainCtx.beginPath();
        mainCtx.moveTo(x-1, 0);
        mainCtx.lineTo(x-1, height);
        mainCtx.stroke();

        mainCtx.beginPath();
        mainCtx.moveTo(x, 0);
        mainCtx.lineTo(x, height);
        mainCtx.stroke();

        mainCtx.beginPath();
        mainCtx.moveTo(x+1, 0);
        mainCtx.lineTo(x+1, height);
        mainCtx.stroke();
      }
    }
  }

  var clap_cnt = 0;
  var combo = 0, score = 0;
  var last_sum = 0;
  function process() {
    // console.log('process()');
    var sum = 0;
    for (var i = 0; i < array.length; i += 1) {
      var value = array[i];
      sum += value;
    }

    tm = Date.now() - start_time - pausedTimes;

    // $('#log').html(sum)
    clap = false;
    if (sum > 40000 && last_sum < sum) {
      clap = true;

      clap_cnt = clap_cnt + 1;
      console.log('CLAP' + clap_cnt);
    }
    last_sum = sum;

    if (clap) {
      // RECORD CLAP
      if (tm - last_time > 100) {
        console.log("RECORD CLAP");
        rec_item.notes.push(tm)

        last_time = tm;
      }
    }
  }


  function clearCanvas() {
    // ctx.clearRect(0, 0, canvasWidth, canvasHeight);
    // x = 0;
  }

  function resizeCanvas() {
    $('#mainCanvas').attr('width', $('#mainCanvas').parent().width());
  }

  $( window ).resize(function() {
    resizeCanvas();
  });

  // 2. This code loads the IFrame Player API code asynchronously.
  var tag = document.createElement('script');

  tag.src = "https://www.youtube.com/iframe_api";
  var firstScriptTag = document.getElementsByTagName('script')[0];
  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

  // 3. This function creates an <iframe> (and YouTube player)
  //    after the API code downloads.
  var player;
  function onYouTubeIframeAPIReady() {
    player = new YT.Player('player', {
      // height: '390',
      width: '100%',
      videoId: '{{ video_id }}',
      playerVars: {
        'controls': 0,
        'disablekb': 1,
        'iv_load_policy': 3,
        'rel': 0
      },
      events: {
        'onReady': onPlayerReady,
        'onStateChange': onPlayerStateChange
      }
    });
  }

  // 4. The API will call this function when the video player is ready.
  function onPlayerReady(event) {
    // event.target.playVideo();
    console.log('onPlayerReady');
    $('#video_title').val(player.getVideoData().title);
  }

  // 5. The API calls this function when the player's state changes.
  //    The function indicates that when playing a video (state=1),
  //    the player should play for six seconds and then stop.
  var done = false;
  function onPlayerStateChange(event) {
    console.log('onPlayerStateChange()');
    if (event.data == -1) {
      var loaded_video_id = player.getVideoData().video_id;
      if (loaded_video_id != "{{ video_id }}") {
        // TODO: ALERT 후 이동
        // location.href = "/yt/" + loaded_video_id;
      }
    }

    if (event.data == YT.PlayerState.PLAYING) {
      if (!done) {
        // 시작
        console.log('Play Start!');
        // setTimeout(stopVideo, 6000);
        done = true;
        startGame();
      } else if (!playing) {
        resumeGame();
      }
    }

    if (event.data == YT.PlayerState.PAUSED || event.data == YT.PlayerState.BUFFERING) {
      pauseGame();
    }

    if (event.data == YT.PlayerState.ENDED) {
      stopGame();
    }
  }

  function stopVideo() {
    player.stopVideo();
  }
</script>
{% endblock %}